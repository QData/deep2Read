---
layout: post
title: Reliable Applications V - Understanding2
desc: 2017-W10
categories:
- 3Reliable
term: 2017Course
tags: [ visualizing, Difference-Analysis, Attribution, Composition]
---


| Presenter | Papers | Paper URL| Our Slides |
| -----: | ---------------------------: | :----- | :----- |
| Rita | Visualizing Deep Neural Network Decisions: Prediction Difference Analysis, ICLR17 [^2] | [PDF](https://arxiv.org/abs/1702.04595) | [PDF]({{site.baseurl}}/talks/20171024-Rita.pdf) |
| Arshdeep | Axiomatic Attribution for Deep Networks, ICML17 [^3] | [PDF](http://proceedings.mlr.press/v70/sundararajan17a/sundararajan17a.pdf) | [PDF]({{site.baseurl}}/talks/20171031-Arshdeep.pdf) |
|  | The Robustness of Estimator Composition, NIPS16 | [PDF](https://arxiv.org/abs/1609.01226) |





[^1]: <sub><sup> Visualizing Deep Neural Network Decisions: Prediction Difference Analysis, ICLR17 / Luisa M Zintgraf, Taco S Cohen, Tameem Adel, Max Welling/ This article presents the prediction difference analysis method for visualizing the response of a deep neural network to a specific input. When classifying images, the method highlights areas in a given input image that provide evidence for or against a certain class. It overcomes several shortcoming of previous methods and provides great additional insight into the decision making process of classifiers. Making neural network decisions interpretable through visualization is important both to improve models and to accelerate the adoption of black-box classifiers in application areas such as medicine. We illustrate the method in experiments on natural images (ImageNet data), as well as medical images (MRI brain scans). </sup></sub>



[^2]: <sub><sup> Axiomatic Attribution for Deep Networks, ICML17 / Google / We study the problem of attributing the prediction of a deep network to its input features, a problem previously studied by several other works. We identify two fundamental axioms---Sensitivity and Implementation Invariance that attribution methods ought to satisfy. We show that they are not satisfied by most known attribution methods, which we consider to be a fundamental weakness of those methods. We use the axioms to guide the design of a new attribution method called Integrated Gradients. Our method requires no modification to the original network and is extremely simple to implement; it just needs a few calls to the standard gradient operator. We apply this method to a couple of image models, a couple of text models and a chemistry model, demonstrating its ability to debug networks, to extract rules from a network, and to enable users to engage with models better. </sup></sub>


[^3]: <sub><sup>  Toward Deeper Understanding of Neural Networks: The Power of Initialization and a Dual View on Expressivity, NIPS16 / GoogleBrain / We develop a general duality between neural networks and compositional kernel Hilbert spaces. We introduce the notion of a computation skeleton, an acyclic graph that succinctly describes both a family of neural networks and a kernel space. Random neural networks are generated from a skeleton through node replication followed by sampling from a normal distribution to assign weights. The kernel space consists of functions that arise by compositions, averaging, and non-linear transformations governed by the skeleton's graph topology and activation functions. We prove that random networks induce representations which approximate the kernel space. In particular, it follows that random weight initialization often yields a favorable starting point for optimization despite the worst-case intractability of training neural networks. </sup></sub>
