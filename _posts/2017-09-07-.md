---
layout: post
title: Theoretical17 IV - Investigating Behaviors of DNN
desc: 2017-W3
categories:
- 1Theoretical
term: 2017Course
tags: [ understanding, black-box, Parsimonious, Associative, memory ]
---

| Presenter | Papers | Paper URL| Our Slides |
| -----: | ---------------------------: | :----- | :----- |
<!--header-->
| Beilun | Learning Deep Parsimonious Representations, NIPS16 [^1] | [PDF](https://papers.nips.cc/paper/6263-learning-deep-parsimonious-representations) | [PDF]({{site.baseurl}}/talks/20170907-Beilun.pdf) |
| Jack | Dense Associative Memory for Pattern Recognition, NIPS16 [^2]| [PDF](https://arxiv.org/abs/1606.01164) + [video](hhttps://www.youtube.com/watch?v=30lMjQk_Lb0) | [PDF]({{site.baseurl}}/talks/20170907-Jack.pdf) |

<!--excerpt.start-->
[^1]: <sub><sup> Learning Deep Parsimonious Representations, NIPS16 / from 	Raquel Urtasun et al. / In this paper we aim at facilitating generalization for deep networks while supporting interpretability of the learned representations. Towards this goal, we propose a clustering based regularization that encourages parsimonious representations. Our k-means style objective is easy to optimize and flexible, supporting various forms of clustering, such as sample clustering, spatial clustering, as well as co-clustering. We demonstrate the effectiveness of our approach on the tasks of unsupervised learning, classification, fine grained categorization, and zero-shot learning. </sup></sub>





[^2]: <sub><sup>  Dense Associative Memory for Pattern Recognition, NIPS16 / from Hopfield/ A model of associative memory is studied, which stores and reliably retrieves many more patterns than the number of neurons in the network. We propose a simple duality between this dense associative memory and neural networks commonly used in deep learning. On the associative memory side of this duality, a family of models that smoothly interpolates between two limiting cases can be constructed. One limit is referred to as the feature-matching mode of pattern recognition, and the other one as the prototype regime. On the deep learning side of the duality, this family corresponds to feedforward neural networks with one hidden layer and various activation functions, which transmit the activities of the visible neurons to the hidden layer. This family of activation functions includes logistics, rectified linear units, and rectified polynomials of higher degrees. The proposed duality makes it possible to apply energy-based intuition from associative memory to analyze computational properties of neural networks with unusual activation functions - the higher rectified polynomials which until now have not been used in deep learning. The utility of the dense memories is illustrated for two test cases: the logical gate XOR and the recognition of handwritten digits from the MNIST data set. </sup></sub>





