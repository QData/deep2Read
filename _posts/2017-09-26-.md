---
layout: post
title: Structure III - DNN with Attention
desc: 2017-W6
tags:
- 2Structures
categories: 2017Course
---




| Presenter | Papers | Paper URL| Our Presentation |
| -----: | ---------------------------: | :----- | :----- |
| Rita | Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer, ICLR17 [^1] |  [PDF](https://arxiv.org/abs/1612.03928)| [PDF]({{site.baseurl}}/talks/20170926-Rita.pdf) |
| Tianlu  | Dynamic Coattention Networks For Question Answering, ICLR17 [^2]| [PDF](https://arxiv.org/abs/1611.01604) + [code](https://github.com/marshmelloX/dynamic-coattention-network)| [PDF]({{site.baseurl}}/talks/20170926-Tianlu.pdf) |
| ChaoJiang | Structured Attention Networks, ICLR17 [^3] |[PDF](https://arxiv.org/abs/1702.00887) + [code](https://github.com/harvardnlp/struct-attn) | [PDF]({{site.baseurl}}/talks/20170928-Chao.pdf) |


[^1]: <sub><sup>  Paying More Attention to Attention: Improving the Performance of Convolutional Neural Networks via Attention Transfer, ICLR17 / Attention plays a critical role in human visual experience. Furthermore, it has recently been demonstrated that attention can also play an important role in the context of applying artificial neural networks to a variety of tasks from fields such as computer vision and NLP. In this work we show that, by properly defining attention for convolutional neural networks, we can actually use this type of information in order to significantly improve the performance of a student CNN network by forcing it to mimic the attention maps of a powerful teacher network. To that end, we propose several novel methods of transferring attention, showing consistent improvement across a variety of datasets and convolutional neural network architectures. Code and models for our experiments are available at this https [URL](https://github.com/szagoruyko/attention-transfer). </sup></sub>



[^2]: <sub><sup>  Dynamic Coattention Networks For Question Answering, ICLR17 / Caiming Xiong, Victor Zhong, Richard Socher/ Several deep learning models have been proposed for question answering. However, due to their single-pass nature, they have no way to recover from local maxima corresponding to incorrect answers. To address this problem, we introduce the Dynamic Coattention Network (DCN) for question answering. The DCN first fuses co-dependent representations of the question and the document in order to focus on relevant parts of both. Then a dynamic pointing decoder iterates over potential answer spans. This iterative procedure enables the model to recover from initial local maxima corresponding to incorrect answers. On the Stanford question answering dataset, a single DCN model improves the previous state of the art from 71.0% F1 to 75.9%, while a DCN ensemble obtains 80.4% F1. </sup></sub>


[^3]: <sub><sup>  Structured Attention Networks, ICLR17 / Attention networks have proven to be an effective approach for embedding categorical inference within a deep neural network. However, for many tasks we may want to model richer structural dependencies without abandoning end-to-end training. In this work, we experiment with incorporating richer structural distributions, encoded using graphical models, within deep networks. We show that these structured attention networks are simple extensions of the basic attention procedure, and that they allow for extending attention beyond the standard soft-selection approach, such as attending to partial segmentations or to subtrees. We experiment with two different classes of structured attention networks: a linear-chain conditional random field and a graph-based parsing model, and describe how these models can be practically implemented as neural network layers. Experiments show that this approach is effective for incorporating structural biases, and structured attention networks outperform baseline attention models on a variety of synthetic and real tasks: tree transduction, neural machine translation, question answering, and natural language inference. We further find that models trained in this way learn interesting unsupervised hidden representations that generalize simple attention. </sup></sub>