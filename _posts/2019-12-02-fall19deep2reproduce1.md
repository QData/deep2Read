---
layout: post
title: deep2reproduce 2019 Fall - 1Analysis papers 
desc: 2019-fall Students deep2reproduce Course Project
term: 2019fCourse
categories:
- 1Theoretical
tags: [ analysis, generalization, forgetting, training, optimization, subspace, informax, normalization, Sample-selection  ]
---


{% assign filedir = site.baseurl   | append: '/deep2reproduce/2019Fall/' %}



|Team INDEX     |Title  & Link  | Our Slide |  Tags | 
|------|----------------------------|----------|----------|
|2   | [Empirical Study of Example Forgetting During Deep Neural Network Learning](https://arxiv.org/abs/1812.05159)|                [OurSlide]({{filedir}}/T2-Pattarabanjird_Tanyaporn_Empirical_Study_of_Example_Forgetting_During_Deep_Neural_Network_Learning.pdf) | Sample-selection, forgetting |
|9   | [How SGD Selects the Global Minima in over-parameterized Learning](https://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective)|  [OurSlide]({{filedir}}/T9_Bamrara_Rishabrb6xj_How_SGD_Selects_the_Global_Minima.pdf) |  optimization |
|10   | [ Escaping Saddles with Stochastic Gradients ](https://arxiv.org/abs/1803.05999)|  [OurSlide]({{filedir}}/T10_kd4wa+dc9db+yl5nx+an2adv_ESCAPING_SADDLES.pdf) |  optimization |
|13   | [To What Extent Do Different Neural Networks Learn the Same Representation](https://arxiv.org/abs/1810.11750)|  [OurSlide]({{filedir}}/T13_Sudhakar_Mohitms5sw_Do_Different_Neural_Networks_Learn_the_Same_Representation.pdf) |  subspace |
|19   | [On the Information Bottleneck Theory of Deep Learning](https://openreview.net/forum?id=ry_WPG-A-)|  [OurSlide]({{filedir}}/T19_Luo_Zhidanzl6de_INFORMATION_BOTTLENECK.pdf) |  informax |
|20   | [Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913)|  [OurSlide]({{filedir}}/T20_Du_Yuyd2am_Visualizing_the_Loss_Landscape.pdf) | normalization  |
|21   | [Using Pre-Training Can Improve Model Robustness and Uncertainty ](https://arxiv.org/abs/1901.09960)|  [OurSlide]({{filedir}}/T21_Wang_Clarerw9fs_Pre-Training_Can_Improve_Model_Robustness_and_Uncertainty.pdf) | training, analysis |
|24   | [Norm matters: efficient and accurate normalization schemes in deep networks](https://arxiv.org/abs/1803.01814)|  [OurSlide]({{filedir}}/T24_Peddireddy_Akhil_Saiap3ub_Norm_Matters.pdf) | normalization  |
|27   | [Implicit Acceleration by Overparameterization](https://arxiv.org/abs/1802.06509)|  [OurSlide]({{filedir}}/T27_Luo_Tianyangtl2sf_Implicit_Acceleration_by_Overparameterization.pdf) | optimization, analysis |
|29  | [Select Via Proxy: Efficient Data Selection For Training Deep Networks](https://arxiv.org/abs/1906.11829)|  [OurSlide]({{filedir}}/T29_Cascante_Bonilla_Paolapc9za_Select_Via_Proxy_data4train.pdf) | Sample-selection|


