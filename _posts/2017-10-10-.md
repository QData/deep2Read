---
layout: post
title: Reliable Applications I - Understanding
desc: 2017-W8
tags:
- 3Reliable
categories: 2017Course
---



| Presenter | Papers | Paper URL| Our Presentation |
| -----: | ---------------------------: | :----- | :----- |
| Rita | Learning Important Features Through Propagating Activation Differences, ICML17 | [PDF](https://arxiv.org/abs/1704.02685) | [PDF]({{site.baseurl}}/talks/20171010-Rita.pdf) |
| GaoJi  | Examples are not Enough, Learn to Criticize! Model Criticism for Interpretable Machine Learning, NIPS16 | [PDF](http://people.csail.mit.edu/beenkim/papers/KIM2016NIPS_MMD.pdf) | [PDF]({{site.baseurl}}/talks/20171010-Ji.pdf) |
| Rita | Learning Kernels with Random Features, Aman Sinha*; John Duchi, | [PDF](https://stanford.edu/~jduchi/projects/SinhaDu16.pdf) | [PDF]({{site.baseurl}}/talks/20170907-Rita.pdf) |


[^1]: <sub><sup> Learning Kernels with Random Features / John Duchi NIPS2016/ Randomized features provide a computationally efficient way to approximate kernel machines in machine learning tasks. However, such methods require a user-defined kernel as input. We extend the randomized-feature approach to the task of learning a kernel (via its associated random features). Specifically, we present an efficient optimization problem that learns a kernel in a supervised manner. We prove the consistency of the estimated kernel as well as generalization bounds for the class of estimators induced by the optimized kernel, and we experimentally evaluate our technique on several datasets. Our approach is efficient and highly scalable, and we attain competitive results with a fraction of the training cost of other techniques. </sup></sub>



[^2]: <sub><sup>  Learning Important Features Through Propagating Activation Differences, ICML17 / DeepLIFE / The purported "black box"' nature of neural networks is a barrier to adoption in applications where interpretability is essential. Here we present DeepLIFT (Deep Learning Important FeaTures), a method for decomposing the output prediction of a neural network on a specific input by backpropagating the contributions of all neurons in the network to every feature of the input. DeepLIFT compares the activation of each neuron to its 'reference activation' and assigns contribution scores according to the difference. By optionally giving separate consideration to positive and negative contributions, DeepLIFT can also reveal dependencies which are missed by other approaches. Scores can be computed efficiently in a single backward pass. We apply DeepLIFT to models trained on MNIST and simulated genomic data, and show significant advantages over gradient-based methods. A detailed video tutorial on the method is at this http URL and code is at this http URL. </sup></sub>


[^3]: <sub><sup>  Examples are not Enough, Learn to Criticize! Model Criticism for Interpretable Machine Learning, NIPS16 / Been Kim et al. / Example-based explanations are widely used in the effort to improve the interpretability of highly complex distributions. However, prototypes alone are rarely sufficient to represent the gist of the complexity. In order for users to construct better mental models and understand complex data distributions, we also need criticism to explain what are not captured by prototypes. Motivated by the Bayesian model criticism framework, we develop MMD-critic which efficiently learns prototypes and criticism, designed to aid human interpretability. A human subject pilot study shows that the MMD-critic selects prototypes and criticism that are useful to facilitate human understanding and reasoning. We also evaluate the prototypes selected by MMD-critic via a nearest prototype classifier, showing competitive performance compared to baselines. </sup></sub>
