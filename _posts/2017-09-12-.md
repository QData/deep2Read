---
layout: post
title: Foundations V - More about Behaviors of DNN
desc: 2017-W4
tags:
- 1Foundations
categories: 2017Course
---


| Presenter | Papers | Information| OurPresentation |
| -----: | ----------: | :----- | :----- |
|  Ceyer | A Closer Look at Memorization in Deep Networks, ICML17 | [PDF](https://arxiv.org/pdf/1706.05394.pdf) | [PDF]({{site.baseurl}}/talks/20170912-Ceyer.pdf) |
|  | On the Expressive Efficiency of Overlapping Architectures of Deep Learning | [DLSSpdf](https://drive.google.com/file/d/0B6NHiPcsmak1ZzVkci1EdVN2YkU/view?usp=drive_web) + [video](http://videolectures.net/deeplearning2017_sharir_deep_learning/) |



> ####  A Closer Look at Memorization in Deep Networks, ICML17
>> from etc Aaron Courville, Yoshua Bengio, Simon Lacoste-Julien / We examine the role of memorization in deep learning, drawing connections to capacity, generalization, and adversarial robustness. While deep networks are capable of memorizing noise data, our results suggest that they tend to prioritize learning simple patterns first. In our experiments, we expose qualitative differences in gradient-based optimization of deep neural networks (DNNs) on noise vs. real data. We also demonstrate that for appropriately tuned explicit regularization (e.g., dropout) we can degrade DNN training performance on noise datasets without compromising generalization on real data. Our analysis suggests that the notions of effective capacity which are dataset independent are unlikely to explain the generalization performance of deep networks when trained with gradient based methods because training data itself plays an important role in determining the degree of memorization.






> ####  On the Expressive Efficiency of Overlapping Architectures of Deep Learning  / ICLR 2018
>> Expressive efficiency refers to the relation between two architectures A and B, whereby any function realized by B could be replicated by A, but there exists functions realized by A, which cannot be replicated by B unless its size grows significantly larger. For example, it is known that deep networks are exponentially efficient with respect to shallow networks, in the sense that a shallow network must grow exponentially large in order to approximate the functions represented by a deep network of polynomial size. In this work, we extend the study of expressive efficiency to the attribute of network connectivity and in particular to the effect of "overlaps" in the convolutional process, i.e., when the stride of the convolution is smaller than its filter size (receptive field). To theoretically analyze this aspect of network's design, we focus on a well-established surrogate for ConvNets called Convolutional Arithmetic Circuits (ConvACs), and then demonstrate empirically that our results hold for standard ConvNets as well. Specifically, our analysis shows that having overlapping local receptive fields, and more broadly denser connectivity, results in an exponential increase in the expressive capacity of neural networks. Moreover, while denser connectivity can increase the expressive capacity, we show that the most common types of modern architectures already exhibit exponential increase in expressivity, without relying on fully-connected layers.


