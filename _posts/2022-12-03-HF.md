---
desc: 2022-W5
term: 2022-selfRead
title: RLHF + InstructGPT  
categories:
- 6Reinforcement
tags: [ RL, AGI, language model, Human Alignment ]  
---



| Papers | Paper URL| Abstract | 
| -----------------------: | :------------ | :------------------------- | 
| Training language models to follow instructions with human feedback | [ URL](https://arxiv.org/abs/2203.02155) | "further fine-tune this supervised model using reinforcement learning from human feedback. We call the resulting models InstructGPT." |
| Deep reinforcement learning from human preferences | [  URL](https://openreview.net/forum?id=GisHNaleWiA) | "explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function" |


