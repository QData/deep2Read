---
layout: post
title: Structure IV - DNN with Attention 2
desc: 2017-W6
tags:
- 2Structures
categories: 2017Course
---



| Presenter | Papers | Information| OurPresentation |
| -----: | ----------: | :----- | :----- |
| Jack  |  Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain, ICLR17 | [PDF](https://arxiv.org/abs/1510.02879)| [PDF]({{site.baseurl}}/talks/20170928-Jack.pdf) |
| Arshdeep | Bidirectional Attention Flow for Machine Comprehension, ICLR17 | [PDF](https://arxiv.org/abs/1611.01603) + [code](https://github.com/allenai/bi-att-flow)| [PDF]({{site.baseurl}}/talks/20170928-Arshdeep.pdf) |
| Ceyer | Image-to-Markup Generation with Coarse-to-Fine Attention, ICML17 |[PDF](http://lstm.seas.harvard.edu/latex/) + [code](https://github.com/harvardnlp/im2markup) | [PDF]({{site.baseurl}}/talks/20170928-Ceyer.pdf) |
| ChaoJiang |  Can Active Memory Replace Attention? ; Samy Bengio, NIPS16 | [PDF](https://arxiv.org/abs/1610.08613)  | [PDF]({{site.baseurl}}/talks/20171003-Chao.pdf) |
|  | An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax, ICLR17 | [PDF](https://arxiv.org/abs/1611.01886)|
