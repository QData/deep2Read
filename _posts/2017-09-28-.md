---
layout: post
title: Structure IV - DNN with Attention 2
desc: 2017-W6
categories:
- 2Architecture
term: 2017Course
tags: [ attention, transfer-learning, relational, generative, memory, Infomax]
---



| Presenter | Papers | Paper URL| Our Slides |
| -----: | ---------------------------: | :----- | :----- |
| Jack  |  Attend, Adapt and Transfer: Attentive Deep Architecture for Adaptive Transfer from multiple sources in the same domain, ICLR17 [^1]| [PDF](https://arxiv.org/abs/1510.02879)| [PDF]({{site.baseurl}}/talks/20170928-Jack.pdf) |
| Arshdeep | Bidirectional Attention Flow for Machine Comprehension, ICLR17 [^2] | [PDF](https://arxiv.org/abs/1611.01603) + [code](https://github.com/allenai/bi-att-flow)| [PDF]({{site.baseurl}}/talks/20170928-Arshdeep.pdf) |
| Ceyer | Image-to-Markup Generation with Coarse-to-Fine Attention, ICML17 |[PDF](http://lstm.seas.harvard.edu/latex/) + [code](https://github.com/harvardnlp/im2markup) | [PDF]({{site.baseurl}}/talks/20170928-Ceyer.pdf) |
| ChaoJiang |  Can Active Memory Replace Attention? ; Samy Bengio, NIPS16 [^3]| [PDF](https://arxiv.org/abs/1610.08613)  | [PDF]({{site.baseurl}}/talks/20171003-Chao.pdf) |
|  | An Information-Theoretic Framework for Fast and Robust Unsupervised Learning via Neural Population Infomax, ICLR17 | [PDF](https://arxiv.org/abs/1611.01886)|



[^1]: <sub><sup>  Bidirectional Attention Flow for Machine Comprehension, ICLR17 /Machine comprehension (MC), answering a query about a given context paragraph, requires modeling complex interactions between the context and the query. Recently, attention mechanisms have been successfully extended to MC. Typically these methods use attention to focus on a small portion of the context and summarize it with a fixed-size vector, couple attentions temporally, and/or often form a uni-directional attention. In this paper we introduce the Bi-Directional Attention Flow (BIDAF) network, a multi-stage hierarchical process that represents the context at different levels of granularity and uses bi-directional attention flow mechanism to obtain a query-aware context representation without early summarization. Our experimental evaluations show that our model achieves the state-of-the-art results in Stanford Question Answering Dataset (SQuAD) and CNN/DailyMail cloze test. </sup></sub>



[^2]: <sub><sup>  Image-to-Markup Generation with Coarse-to-Fine Attention, ICML17/ We present a neural encoder-decoder model to convert images into presentational markup based on a scalable coarse-to-fine attention mechanism. Our method is evaluated in the context of image-to-LaTeX generation, and we introduce a new dataset of real-world rendered mathematical expressions paired with LaTeX markup. We show that unlike neural OCR techniques using CTC-based models, attention-based approaches can tackle this non-standard OCR task. Our approach outperforms classical mathematical OCR systems by a large margin on in-domain rendered data, and, with pretraining, also performs well on out-of-domain handwritten data. To reduce the inference complexity associated with the attention-based approaches, we introduce a new coarse-to-fine attention layer that selects a support region before applying attention. </sup></sub>


[^3]: <sub><sup>  Can Active Memory Replace Attention? ; Samy Bengio, NIPS16/ Several mechanisms to focus attention of a neural network on selected parts of its input or memory have been used successfully in deep learning models in recent years. Attention has improved image classification, image captioning, speech recognition, generative models, and learning algorithmic tasks, but it had probably the largest impact on neural machine translation. Recently, similar improvements have been obtained using alternative mechanisms that do not focus on a single part of a memory but operate on all of it in parallel, in a uniform way. Such mechanism, which we call active memory, improved over attention in algorithmic tasks, image processing, and in generative modelling. So far, however, active memory has not improved over attention for most natural language processing tasks, in particular for machine translation. We analyze this shortcoming in this paper and propose an extended model of active memory that matches existing attention models on neural machine translation and generalizes better to longer sentences. We investigate this model and explain why previous active memory models did not succeed. Finally, we discuss when active memory brings most benefits and where attention can be a better choice. </sup></sub>
