---
layout: post
title: deep2reproduce 2019 Fall - 1Analysis papers 
desc: 2019-fall Students deep2reproduce 
term: 2019fCourse
categories:
- 1Theoretical
tags: [ analysis, generalization, forgetting, training, optimization, subspace, informax, normalization, Sample-selection  ]
---


{% assign filedir = site.baseurl   | append: '/deep2reproduce/2019Fall/' %}



|Team INDEX     |Title  & Link  |  Tags |  Our Slide | 
|------|----------------------------|----------|----------|
|T2   | [Empirical Study of Example Forgetting During Deep Neural Network Learning](https://arxiv.org/abs/1812.05159)|  Sample Selection, forgetting |[OurSlide]({{filedir}}/T2-Pattarabanjird_Tanyaporn_Empirical_Study_of_Example_Forgetting_During_Deep_Neural_Network_Learning.pdf) | 
|T29  | [Select Via Proxy: Efficient Data Selection For Training Deep Networks](https://arxiv.org/abs/1906.11829)| Sample Selection| [OurSlide]({{filedir}}/T29_Cascante_Bonilla_Paolapc9za_Select_Via_Proxy_data4train.pdf) | 
|T9   | [How SGD Selects the Global Minima in over-parameterized Learning](https://papers.nips.cc/paper/8049-how-sgd-selects-the-global-minima-in-over-parameterized-learning-a-dynamical-stability-perspective)|  optimization | [OurSlide]({{filedir}}/T9_Bamrara_Rishabrb6xj_How_SGD_Selects_the_Global_Minima.pdf) | 
|T10   | [ Escaping Saddles with Stochastic Gradients ](https://arxiv.org/abs/1803.05999)| optimization | [OurSlide]({{filedir}}/T10_kd4wa+dc9db+yl5nx+an2adv_ESCAPING_SADDLES.pdf) |  
|T13   | [To What Extent Do Different Neural Networks Learn the Same Representation](https://arxiv.org/abs/1810.11750)| subspace |  [OurSlide]({{filedir}}/T13_Sudhakar_Mohitms5sw_Do_Different_Neural_Networks_Learn_the_Same_Representation.pdf) |  
|T19   | [On the Information Bottleneck Theory of Deep Learning](https://openreview.net/forum?id=ry_WPG-A-)| informax | [OurSlide]({{filedir}}/T19_Luo_Zhidanzl6de_INFORMATION_BOTTLENECK.pdf) |  
|T20   | [Visualizing the Loss Landscape of Neural Nets](https://arxiv.org/abs/1712.09913)| normalization  | [OurSlide]({{filedir}}/T20_Du_Yuyd2am_Visualizing_the_Loss_Landscape.pdf) | 
|T21   | [Using Pre-Training Can Improve Model Robustness and Uncertainty ](https://arxiv.org/abs/1901.09960)| training, analysis | [OurSlide]({{filedir}}/T21_Wang_Clarerw9fs_Pre-Training_Can_Improve_Model_Robustness_and_Uncertainty.pdf) | 
|T24   | [Norm matters: efficient and accurate normalization schemes in deep networks](https://arxiv.org/abs/1803.01814)| normalization  | [OurSlide]({{filedir}}/T24_Peddireddy_Akhil_Saiap3ub_Norm_Matters.pdf) | 


