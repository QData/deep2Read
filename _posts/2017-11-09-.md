---
toc: true
author_profile: true
sidebar:
  title: "Reviews Indexed"
  nav: sidebar-sample
layout: single
title: Optimization IV -   change DNN architecture for Optimization 
desc: 2017-W12
categories:
- 4Optimization
term: 2017Course
tags: [ Forcing,  Optimization]
---


| Presenter | Papers | Paper URL| Our Slides |
| -----: | ---------------------------: | :----- | :----- |
| Shijia | Professor Forcing: A New Algorithm for Training Recurrent Networks, [^1] NIPS16 | [PDF](https://arxiv.org/abs/1610.09038) + [Video](http://videolectures.net/deeplearning2016_goyal_new_algorithm/)| [PDF]({{site.baseurl}}/talks2017/20171109-Shijia.pdf) |
| Beilun+Arshdeep |  Mollifying Networks, Bengio, ICLR17  [^2]| [PDF](https://arxiv.org/abs/1608.04980) | [PDF]({{site.baseurl}}/talks2017/20171109-Arshdeep.pdf) / [PDF2]({{site.baseurl}}/talks2017/20171109-BeilunArshdeep.pdf) |

<!--excerpt.start-->
[^1]: <sub><sup> Mollifying Networks, Bengio, ICLR17/ The optimization of deep neural networks can be more challenging than traditional convex optimization problems due to the highly non-convex nature of the loss function, e.g. it can involve pathological landscapes such as saddle-surfaces that can be difficult to escape for algorithms based on simple gradient descent. In this paper, we attack the problem of optimization of highly non-convex neural networks by starting with a smoothed -- or mollified -- objective function that gradually has a more non-convex energy landscape during the training. Our proposition is inspired by the recent studies in continuation methods: similar to curriculum methods, we begin learning an easier (possibly convex) objective function and let it evolve during the training, until it eventually goes back to being the original, difficult to optimize, objective function. The complexity of the mollified networks is controlled by a single hyperparameter which is annealed during the training. We show improvements on various difficult optimization tasks and establish a relationship with recent works on continuation methods for neural networks and mollifiers. </sup></sub>




[^2]: <sub><sup>  Professor Forcing: A New Algorithm for Training Recurrent Networks, NIPS16/ The Teacher Forcing algorithm trains recurrent networks by supplying observed sequence values as inputs during training and using the network's own one-step-ahead predictions to do multi-step sampling. We introduce the Professor Forcing algorithm, which uses adversarial domain adaptation to encourage the dynamics of the recurrent network to be the same when training the network and when sampling from the network over multiple time steps. We apply Professor Forcing to language modeling, vocal synthesis on raw waveforms, handwriting generation, and image generation. Empirically we find that Professor Forcing acts as a regularizer, improving test likelihood on character level Penn Treebank and sequential MNIST. We also find that the model qualitatively improves samples, especially when sampling for a large number of time steps. This is supported by human evaluation of sample quality. Trade-offs between Professor Forcing and Scheduled Sampling are discussed. We produce T-SNEs showing that Professor Forcing successfully makes the dynamics of the network during training and sampling more similar. </sup></sub>

