---
desc: 2022-W4
term: 2022-selfRead
title: Emergent Abilities of LLM 
categories:
- FMBasic
tags: [ language model ]  
---



## Emergent Abilities of Large Language Models 
+  [ URL](https://arxiv.org/abs/2206.07682) 
+ "an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models."


## Language Models are Few-Shot Learners 
+ [ URL](https://arxiv.org/abs/2005.14165) 
+ "GPT-3, 175B autoregerssive LLM;  show that scaling up language models greatly improves task-agnostic, few-shot performance, sometimes even reaching competitiveness with prior state-of-the-art fine-tuning approaches."

## On the Opportunities and Risks of Foundation Models 
+ [  URL](https://arxiv.org/abs/2108.07258)   
+ " a thorough account of the opportunities and risks of foundation models, ranging from their capabilities (e.g., language, vision, robotics, reasoning, human interaction) and technical principles(e.g., model architectures, training procedures, data, systems, security, evaluation, theory) to their applications (e.g., law, healthcare, education) and societal impact (e.g., inequity, misuse, economic and environmental impact, legal and ethical considerations)." |


## The Power of Scale for Parameter-Efficient Prompt Tuning
+ https://arxiv.org/abs/2104.08691
+ Brian Lester, Rami Al-Rfou, Noah Constant
+ In this work, we explore "prompt tuning", a simple yet effective mechanism for learning "soft prompts" to condition frozen language models to perform specific downstream tasks. Unlike the discrete text prompts used by GPT-3, soft prompts are learned through backpropagation and can be tuned to incorporate signal from any number of labeled examples. Our end-to-end learned approach outperforms GPT-3's "few-shot" learning by a large margin. More remarkably, through ablations on model size using T5, we show that prompt tuning becomes more competitive with scale: as models exceed billions of parameters, our method "closes the gap" and matches the strong performance of model tuning (where all model weights are tuned). This finding is especially relevant in that large models are costly to share and serve, and the ability to reuse one frozen model for multiple downstream tasks can ease this burden. Our method can be seen as a simplification of the recently proposed "prefix tuning" of Li and Liang (2021), and we provide a comparison to this and other similar approaches. Finally, we show that conditioning a frozen model with soft prompts confers benefits in robustness to domain transfer, as compared to full model tuning.
