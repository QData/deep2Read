---
layout: post
title: GNN Basics and Applications   
desc: 2019-W4
categories: 2019sCourse
tags:
- 7Graphs
tricks: attention, pairwise, deep-visualization, 3D, CNN  
---


| Presenter | Papers | Paper URL| Our Slides |
| -----: | -------------------------------------: | :----- | :----- |
| Bio |   Protein Interface Prediction using Graph Convolutional Networks   | [Pdf](https://papers.nips.cc/paper/7231-protein-interface-prediction-using-graph-convolutional-networks.pdf) | Eli [Pdf]() | Jack [Pdf]() | 
|  Bio |  KDEEP: Protein–Ligand Absolute Binding Affinity Prediction via 3D-Convolutional Neural Networks, 2018 [^2] |  [Pdf](https://pubs.acs.org/doi/abs/10.1021/acs.jcim.7b00650) | Eli [Pdf]() | Jack [Pdf]() | 
|  Bio |  Molecular geometry prediction using a deep generative graph neural network  | [Pdf](https://arxiv.org/abs/1904.00314) | Eli [Pdf]() | Jack [Pdf]() |
| Program |  Learning to represent programs with graphs | [Pdf](https://arxiv.org/abs/1812.04064) [^1]| Weilin [Pdf]() | Jack [Pdf]() | 
| Bio | Visualizing convolutional neural network protein-ligand scoring | &#9745; Eli | | 
| Bio | Deep generative models of genetic variation capture mutation effects |  &#9745; Eli   |  |  
| Bio |  Attentive cross-modal paratope prediction   [Pdf](https://openreview.net/forum?id=ByUU2t1PG) | &#9745; Eli |  |  


[^1] Jack Note: Many recent works have tried NLP or CV methods to learn representations for predictive models on source code. However, these methods don't fit this data type. The main motivation here is that source code is actually a graph representation, not sequential or local. We can represent program source code as graphs and use different edge types to model syntactic and semantic relationships between different tokens of the program. To do this, we can use program’s abstract syntax tree (AST), consisting of syntax nodes and syntax tokens.  Thus, the hypothesis is that we can use use graph-based deep learning methods to learn to reason over program structures. This paper proposes to use graphs to represent both the syntactic and semantic structure of code and use graph-based deep learning methods to learn to reason over program structures. In addition, they explore how to scale Gated Graph Neural Networks training to such large graphs. We evaluate our method on two tasks: VarMisuse, in which a network attempts to predict the name of a variable given its usage, and VarNaming, in which the network learns to reason about selecting the correct variable that should be used at a given program location. Our comparison to methods that use less structured program representations shows the advantages of modeling known structure, and suggests that our models learn to infer meaningful names and to solve the VarMisuse task in many cases. Additionally, our testing showed that VarNaming identifies a number of bugs in mature open-source projects.



[^2] Jack Note:  Accurately predicting protein−ligand binding affinities is an important problem in computational chemistry since it can substantially accelerate drug discovery for virtual screening and lead optimization. This paper proposes using 3D-CNNs for predicting binding affinities across several diverse datasets. The main idea is they represent the protein and ligand together using their 3D voxel representation. This complex representation of the protein-ligand together is fed through a 3D-CNN to produce a scalar affinity value. It is trained using MSE on the ground truth affinity values. The authors use 4 datasets: PDB containing 58 and 290 complexes, CSAR NRC-HiQ containing 167 and 176 complexes, CSAR2012 containing 57, and CSAR2014 containing 47.   The authors use a 3D voxel representation of both proteins and ligand using a van der Waals radius for each atom type, which in turns gets assigned to a particular property channel (hydrophobic, hydrogen-bond donor or acceptor, aromatic, positive or negative ionizable, metallic and total excluded volume), according to certain rules. The contribution of each atom to each grid point depends on their Euclidean distance. They duplicate the number of properties to account for both protein and ligand, by using the same ones in each, resulting in up to a total of 16 different channels. Their 3D-CNN performed well compared to previous methods and resulted in speed increases due to the parallelization of the GPU.  However, it seems the biggest concern is the representation of the protein-ligand complex considering a specific docking tool is needed to specify how the protein and ligan and linked in the voxel space. I feel this severely prohibits the model when training considering no perturbations of the docking are used. On a similar note, it's very hard to define ``negative'' samples in this task, and I'm curious to see how their model would predict a completely incorrect, or negative protein-ligand complex.
