---
layout: post
title: Structures17 -Adaptive Deep Networks I
desc: 2017-team
tags:
- 2Structures
categories: 2017Reads
---



| Presenter | Papers | Paper URL| OurPresentation |
| -----: | ---------------------------: | :----- | :----- |
| Arshdeep |  HyperNetworks, David Ha, Andrew Dai, Quoc V. Le ICLR 2017 | [PDF](https://arxiv.org/abs/1609.09106) |  [PDF]({{site.baseurl}}/MoreTalksTeam/Arsh/17-01-hypernetworks_parameter_prediction.pdf) | 
| Arshdeep |  Learning feed-forward one-shot learners | [PDF](https://arxiv.org/abs/1606.05233) |  [PDF]({{site.baseurl}}/MoreTalksTeam/Arsh/17-02-one_shot_parameter_prediction.pdf) | 
| Arshdeep |  Learning to Learn by gradient descent by gradient descent | [PDF](https://arxiv.org/abs/1606.04474) |  [PDF]({{site.baseurl}}/MoreTalksTeam/Arsh/17-03-gradient_descent_parameter_prediction.pdf) | 
|  Arshdeep | Dynamic Filter Networks https://arxiv.org/abs/1605.09673 | [PDF](https://arxiv.org/abs/1605.09673) |  [PDF]({{site.baseurl}}/MoreTalksTeam/Arsh/17-04-dynamic-filter-networks.pdf) | 


> #### > HyperNetworks, David Ha, Andrew Dai, Quoc V. Le ICLR 2017 
>> This work explores hypernetworks: an approach of using a one network, also known as a hypernetwork, to generate the weights for another network. Hypernetworks provide an abstraction that is similar to what is found in nature: the relationship between a genotype - the hypernetwork - and a phenotype - the main network. Though they are also reminiscent of HyperNEAT in evolution, our hypernetworks are trained end-to-end with backpropagation and thus are usually faster. The focus of this work is to make hypernetworks useful for deep convolutional networks and long recurrent networks, where hypernetworks can be viewed as relaxed form of weight-sharing across layers. Our main result is that hypernetworks can generate non-shared weights for LSTM and achieve near state-of-the-art results on a variety of sequence modelling tasks including character-level language modelling, handwriting generation and neural machine translation, challenging the weight-sharing paradigm for recurrent networks. Our results also show that hypernetworks applied to convolutional networks still achieve respectable results for image recognition tasks compared to state-of-the-art baseline models while requiring fewer learnable parameters.

> #### > Learning feed-forward one-shot learners, Arxiv  2016, Luca Bertinetto, JoÃ£o F. Henriques, Jack Valmadre, Philip H. S. Torr, Andrea Vedaldi
>> One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark.


> #### > Learning to Learn by gradient descent by gradient descent, Arxiv  2016
>> The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art.



> #### > Dynamic Filter Networks, Bert De Brabandere, Xu Jia, Tinne Tuytelaars, Luc Van Gool
(Submitted on 31 May 2016 (v1), last revised 6 Jun 2016 (this version, v2))
>> In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operations can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture. We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation.
