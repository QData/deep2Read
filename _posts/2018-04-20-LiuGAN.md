---
layout: post
title: Generative18 -Generative Adversarial Network (classified)
desc: 2018-team
tags:
- 5Generative
categories: 2018Reads
---


| Presenter | Papers | Paper URL| Our Presentation |
| -----: | ---------------------------: | :----- | :----- |
| BrandonLiu | Summary of Recent Generative Adversarial Networks (Classified)  |  |  [PDF]({{site.baseurl}}/MoreTalksTeam/Un18/Liu18May-GANSummary.pdf) | 
| Jack |  Generating and designing DNA with deep generative models, Nathan Killoran, Leo J. Lee, Andrew Delong, David Duvenaud, Brendan J. Frey | [PDF](https://arxiv.org/abs/1712.06148) |  [PDF]({{site.baseurl}}/MoreTalksTeam/Jack/20180218_GeneratingDNA.pdf) | 
| GaoJi |  More about basics of GAN |  |  [PDF]({{site.baseurl}}/MoreTalksTeam/Ji/JIGAN.pdf) | 
|  | McGan: Mean and Covariance Feature Matching GAN, PMLR 70:2527-2535 | [PDF](https://arxiv.org/abs/1702.08398) |
|  | Wasserstein GAN, ICML17 | [PDF](https://arxiv.org/abs/1701.07875) |
|  | Geometrical Insights for Implicit Generative Modeling, L Bottou, M Arjovsky, D Lopez-Paz, M Oquab  | [PDF](https://arxiv.org/pdf/1712.07822.pdf) |



> ####  McGan: Mean and Covariance Feature Matching GAN, ICML17, PMLR 70:2527-2535
>> We introduce new families of Integral Probability Metrics (IPM) for training Generative Adversarial Networks (GAN). Our IPMs are based on matching statistics of distributions embedded in a finite dimensional feature space. Mean and covariance feature matching IPMs allow for stable training of GANs, which we will call McGan. McGan minimizes a meaningful loss between distributions.



> ####  Wasserstein GAN, ICML17
>> We introduce a new algorithm named WGAN, an alternative to traditional GAN training. In this new model, we show that we can improve the stability of learning, get rid of problems like mode collapse, and provide meaningful learning curves useful for debugging and hyperparameter searches. Furthermore, we show that the corresponding optimization problem is sound, and provide extensive theoretical work highlighting the deep connections to different distances between distributions.




> ####  Generating and designing DNA with deep generative models, Nathan Killoran, Leo J. Lee, Andrew Delong, David Duvenaud, Brendan J. Frey / 2017 / partial 
>> We propose generative neural network methods to generate DNA sequences and tune them to have desired properties. We present three approaches: creating synthetic DNA sequences using a generative adversarial network; a DNA-based variant of the activation maximization ("deep dream") design method; and a joint procedure which combines these two approaches together. We show that these tools capture important structures of the data and, when applied to designing probes for protein binding microarrays, allow us to generate new sequences whose properties are estimated to be superior to those found in the training data. We believe that these results open the door for applying deep generative models to advance genomics research.


> #### Geometrical Insights for Implicit Generative Modeling
>> Learning algorithms for implicit generative models can optimize a variety of criteria that measure how the data distribution differs from the implicit model distribution, including the Wasserstein distance, the Energy distance, and the Maximum Mean Discrepancy criterion. A careful look at the geometries induced by these distances on the space of probability measures reveals interesting differences. In particular, we can establish surprising approximate global convergence guarantees for the 1-Wasserstein distance,even when the parametric generator has a nonconvex parametrization.

